# AI Hacking and Research Repository

Welcome to my **AI Hacking and Research** repository.  This project is a place to collect resources, notes and hands‑on exercises relating to offensive security for artificial‑intelligence systems.  It complements my current work on AI security capture‑the‑flag exercises such as the challenge series at [gandalf.lakera.ai](https://gandalf.lakera.ai/) by organising training material and references.

Below is an outline of the resources I plan to work through.  Each subsection briefly describes the course, training programme or book.  As I progress I will add more detailed notes, labs, write‑ups and projects under the appropriate section.

## Courses and Learning Paths

### AI Red Teamer Job Role Path – HTB Academy
This job‑role learning path, created by **Hack The Box** in collaboration with **Google**, equips cybersecurity professionals to **assess, exploit and secure AI systems**.  It teaches red‑teaming techniques such as prompt injection, jailbreaking LLMs, and model exploitation while aligning with Google’s Secure AI Framework (SAIF).

### Hands‑On AI (LLM) Red Teaming – Udemy
A practical Udemy course that starts with the basics of large language models and progresses to advanced red‑teaming techniques.  Learners study **LLM fundamentals, the OWASP Top 10 for LLMs, and hands‑on red‑teaming tools** while practising jailbreaking and adversarial prompt engineering.

### A Deep Dive into LLM Red Teaming – Udemy
This course teaches prompt injection, jailbreak tactics, indirect prompt attacks and LLM vulnerability testing from beginner to advanced levels.  It covers **OWASP Top 10 attacks on ML systems**, red‑teaming generative AI and building testing frameworks.

### The Ultimate AI/LLM/ML Penetration Testing Training Course – Udemy
A comprehensive penetration‑testing course for ethical hackers and bug hunters that addresses the **full spectrum of LLM and machine‑learning vulnerabilities**.  Topics include prompt injection, insecure output handling, training‑data poisoning, denial‑of‑service, supply‑chain vulnerabilities, sensitive‑information disclosure, insecure plugin design, excessive agency, overreliance and model theft.

### AI Security – Udemy
This course explores how generative AI intersects with cybersecurity.  Students learn to **write effective prompts, leverage ChatGPT and Microsoft Security Copilot for SOC/CTI tasks**, review the OWASP Top 10 vulnerabilities for LLMs, apply the MITRE ATLAS framework and build governance and threat‑modelling programmes.  It also covers penetration‑testing approaches for generative AI systems.

### AI Red Teaming & AI Security Masterclass
A live masterclass that blends technical depth with practical exercises.  It focuses on prompt injection, AI risk management and large language model security, culminating in a certification.  Participants gain hands‑on experience with AI red‑teaming techniques and defensive strategies.

### LLM Red Teaming: AI Security Testing – OffSec
An OffSec course that teaches how to **assess and test large language models for security risks, abuse cases, prompt injection and model weaknesses**.  The curriculum combines theory with labs and aligns with emerging AI security frameworks.

### Certified AI Security Professional
A professional certification that validates practical AI security skills.  The programme covers assessing model risks, implementing controls and following industry standards for secure AI development and deployment.

### Certified AI GRC Professional – ITCERTS
This certification focuses on **governance, risk management and compliance (GRC)** for AI systems.  It teaches how to evaluate AI risk, ensure regulatory compliance and integrate governance frameworks into AI projects.

### AI Security & Governance Certification – Securiti Education
A certification programme from Securiti Education that emphasises **AI security and governance**.  The curriculum addresses policy development, risk assessment and best practices for securing AI systems in enterprise environments.

### AI for Cybersecurity Course – ISC²
An ISC² course exploring the role of AI in cybersecurity.  It examines **AI‑driven threats, defensive uses of AI and ethical considerations**, preparing professionals to integrate AI into modern security operations.

### ISO/IEC 42001 Artificial Intelligence Management System Training
Training based on the **ISO/IEC 42001** standard for AI management systems.  Participants learn to build and audit management systems that ensure the responsible design, development and operation of AI.

### AI Security Training: Case Studies and Tools for Generative AI – Microsoft
A Microsoft training that uses **case studies and practical tools** to teach security considerations for generative AI.  It covers threat modelling, risk management and the use of Microsoft’s security products for AI applications.

### AI Risk Management for Professionals and Auditors – Udemy
This course introduces AI risk frameworks and auditing techniques.  It equips professionals and auditors to identify, assess and mitigate AI‑related risks and to develop compliance programmes.

### Agentic AI – Risk and Cybersecurity Masterclass 2026 – Udemy
A forward‑looking Udemy masterclass that examines **agentic AI systems** and their associated risks.  It discusses cybersecurity challenges posed by autonomous agents and provides strategies for risk management and governance in 2026 and beyond.

### AI‑102: Microsoft Certified Azure AI Engineer Associate – Udemy
Preparation for the **AI‑102 certification exam**, focusing on designing and implementing AI solutions on Azure.  Topics include natural‑language processing, computer vision, responsible AI practices and security considerations.

### AI Trust, Risk & Security Book Bundle – Humble Tech Bundle (Pearson)
A collection of Pearson books covering **AI trust, risk management and security**.  The bundle provides foundational knowledge and case studies for professionals interested in securing AI systems.

## Books

### Red Team Engineering – No Starch Press
A book that delves into the **engineering principles and methodologies of red teaming**.  It offers strategies for building effective red‑team operations and testing organisational security.

### Red Teaming AI – No Starch Press
This book focuses on **red‑teaming techniques for artificial‑intelligence systems**, explaining how to test and harden AI models against adversarial attacks and prompt‑engineering exploits.

### Practical AI Security – No Starch Press
A practical guide to securing AI systems.  It discusses threat modelling, attack vectors, defensive countermeasures and best practices for deploying AI securely.

### Practical Purple Teaming – No Starch Press
A book that integrates **red‑team and blue‑team** approaches (purple teaming) to improve security posture.  It includes exercises and case studies that highlight collaborative testing between offensive and defensive teams.

---

### How to Use This Repository
Each section above will be expanded with notes, summaries, labs and projects as I work through the material.  Feel free to explore, contribute or suggest additional resources.

### Gandalf Prompt Injection Pentesting

This new subfolder [`gandalf-prompt-injection`](gandalf-prompt-injection/README.md) documents my pentesting exercise against the **gandalf.lakera.ai** challenge. It provides a level-by-level summary of the prompt-injection techniques used to extract the secret password, insights gained, and references to relevant AI security frameworks. The README also outlines guidance on uploading large video files (e.g., levels 6 and 7) using Git LFS or GitHub releases.

